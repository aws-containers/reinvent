# CNS328 | Paper to production: Hosting LLMs on Amazon EKS using NVIDIA GPUs 

Learn how to deploy and operate large language models (LLMs) on Amazon EKS in this practical workshop. You will set up NVIDIA GPU support for inference using Mistral-7B, implement efficient model serving with vLLM's PagedAttention, and scale distributed workloads using Ray Serve. Experience high-performance model storage with FSx for Lustre while implementing comprehensive observability using NVIDIA DCGM and Amazon Managed Service for Prometheus and Grafana to monitor both infrastructure and model performance. You'll discover how Amazon EKS simplifies running LLM workloads at scale. Ideal for ML Engineers and DevOps professionals with basic Kubernetes knowledge, looking to operationalize LLMs.

## Session Resources 

